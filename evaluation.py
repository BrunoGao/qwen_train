#!/usr/bin/env python3
"""
Qwen 2.5 Code æ¨¡å‹è¯„ä¼°æ¡†æ¶
åŒ…å«ä»£ç ç†è§£ã€ç”Ÿæˆã€è¡¥å…¨ç­‰å¤šä¸ªç»´åº¦çš„è¯„ä¼°
"""

import os
import json
import time
import torch
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path

from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
from tqdm import tqdm


@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœæ•°æ®ç»“æ„"""
    task: str
    score: float
    details: Dict[str, Any]
    examples: List[Dict[str, Any]]
    timestamp: str


class CodeEvaluator:
    """ä»£ç æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model_path: str, device: str = "cuda"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        print(f"Loading model from {model_path}...")
        
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="left"
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.model.eval()
        print("Model loaded successfully!")
    
    def generate_response(self, prompt: str, max_length: int = 2048, temperature: float = 0.3) -> str:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä»£ç åˆ†æä¸“å®¶ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·ç†è§£ã€åˆ†æå’Œä¼˜åŒ–ä»£ç ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        # ä½¿ç”¨Qwençš„chat template
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        # æå–ç”Ÿæˆçš„éƒ¨åˆ†
        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()


class CodeUnderstandingEvaluator:
    """ä»£ç ç†è§£èƒ½åŠ›è¯„ä¼°"""
    
    def __init__(self, evaluator: CodeEvaluator):
        self.evaluator = evaluator
        
        # ä»£ç ç†è§£æµ‹è¯•ç”¨ä¾‹
        self.test_cases = [
            {
                "language": "Python",
                "code": """
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)
""",
                "question": "è¯·è§£é‡Šè¿™æ®µä»£ç çš„åŠŸèƒ½å’Œç®—æ³•åŸç†",
                "expected_keywords": ["å¿«é€Ÿæ’åº", "é€’å½’", "åˆ†æ²»", "pivot", "æ—¶é—´å¤æ‚åº¦"]
            },
            {
                "language": "Java",
                "code": """
public class Singleton {
    private static volatile Singleton instance;
    private Singleton() {}
    
    public static Singleton getInstance() {
        if (instance == null) {
            synchronized (Singleton.class) {
                if (instance == null) {
                    instance = new Singleton();
                }
            }
        }
        return instance;
    }
}
""",
                "question": "åˆ†æè¿™ä¸ªè®¾è®¡æ¨¡å¼çš„å®ç°å’Œçº¿ç¨‹å®‰å…¨æ€§",
                "expected_keywords": ["å•ä¾‹æ¨¡å¼", "åŒé‡æ£€æŸ¥", "volatile", "çº¿ç¨‹å®‰å…¨", "æ‡’åŠ è½½"]
            },
            {
                "language": "Go",
                "code": """
package main

import (
    "fmt"
    "sync"
    "time"
)

func worker(id int, jobs <-chan int, results chan<- int, wg *sync.WaitGroup) {
    defer wg.Done()
    for j := range jobs {
        fmt.Printf("worker %d processing job %d\\n", id, j)
        time.Sleep(time.Second)
        results <- j * 2
    }
}

func main() {
    jobs := make(chan int, 100)
    results := make(chan int, 100)
    var wg sync.WaitGroup

    for w := 1; w <= 3; w++ {
        wg.Add(1)
        go worker(w, jobs, results, &wg)
    }

    for j := 1; j <= 9; j++ {
        jobs <- j
    }
    close(jobs)

    wg.Wait()
    close(results)

    for r := range results {
        fmt.Println("result:", r)
    }
}
""",
                "question": "è§£é‡Šè¿™ä¸ªå¹¶å‘ç¨‹åºçš„å·¥ä½œåŸç†",
                "expected_keywords": ["goroutine", "channel", "å¹¶å‘", "å·¥ä½œæ± ", "åŒæ­¥"]
            }
        ]
    
    def evaluate(self) -> EvaluationResult:
        """è¯„ä¼°ä»£ç ç†è§£èƒ½åŠ›"""
        print("Evaluating code understanding...")
        
        total_score = 0
        examples = []
        
        for i, test_case in enumerate(tqdm(self.test_cases, desc="Code Understanding")):
            prompt = f"è¯·åˆ†æä»¥ä¸‹{test_case['language']}ä»£ç ï¼š\n\n```{test_case['language'].lower()}\n{test_case['code']}\n```\n\n{test_case['question']}"
            
            try:
                response = self.evaluator.generate_response(prompt, max_length=1024)
                
                # è¯„ä¼°å“åº”è´¨é‡
                score = self._score_response(response, test_case['expected_keywords'])
                total_score += score
                
                examples.append({
                    "test_case": i + 1,
                    "language": test_case['language'],
                    "prompt": prompt,
                    "response": response,
                    "score": score,
                    "expected_keywords": test_case['expected_keywords']
                })
                
            except Exception as e:
                print(f"Error in test case {i+1}: {e}")
                examples.append({
                    "test_case": i + 1,
                    "error": str(e),
                    "score": 0
                })
        
        avg_score = total_score / len(self.test_cases)
        
        return EvaluationResult(
            task="code_understanding",
            score=avg_score,
            details={
                "total_cases": len(self.test_cases),
                "average_score": avg_score,
                "score_distribution": [ex.get("score", 0) for ex in examples]
            },
            examples=examples,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
    
    def _score_response(self, response: str, expected_keywords: List[str]) -> float:
        """è¯„åˆ†å“åº”è´¨é‡"""
        response_lower = response.lower()
        
        # å…³é”®è¯åŒ¹é…åº¦
        keyword_score = sum(1 for kw in expected_keywords if kw.lower() in response_lower) / len(expected_keywords)
        
        # é•¿åº¦åˆç†æ€§ (50-500å­—ç¬¦)
        length_score = min(1.0, max(0.1, len(response) / 500))
        
        # ç»“æ„åŒ–ç¨‹åº¦ (åŒ…å«æŠ€æœ¯æœ¯è¯­)
        tech_terms = ["å‡½æ•°", "æ–¹æ³•", "ç±»", "å˜é‡", "ç®—æ³•", "å¤æ‚åº¦", "è®¾è®¡æ¨¡å¼", "å¹¶å‘", "åŒæ­¥"]
        structure_score = min(1.0, sum(1 for term in tech_terms if term in response) / 3)
        
        # ç»¼åˆè¯„åˆ†
        final_score = (keyword_score * 0.5 + length_score * 0.2 + structure_score * 0.3)
        return round(final_score, 2)


class CodeGenerationEvaluator:
    """ä»£ç ç”Ÿæˆèƒ½åŠ›è¯„ä¼°"""
    
    def __init__(self, evaluator: CodeEvaluator):
        self.evaluator = evaluator
        
        self.test_cases = [
            {
                "language": "Python",
                "task": "å®ç°ä¸€ä¸ªäºŒå‰æœç´¢æ ‘çš„æ’å…¥å’Œæœç´¢æ–¹æ³•",
                "expected_features": ["class", "def insert", "def search", "self.left", "self.right"]
            },
            {
                "language": "Java", 
                "task": "åˆ›å»ºä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨ç±»",
                "expected_features": ["class", "synchronized", "private", "public", "volatile"]
            },
            {
                "language": "Go",
                "task": "å®ç°ä¸€ä¸ªHTTPå®¢æˆ·ç«¯åŒ…è£…å™¨ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶",
                "expected_features": ["func", "http.Client", "retry", "time.Sleep", "error"]
            },
            {
                "language": "Rust",
                "task": "ç¼–å†™ä¸€ä¸ªå®‰å…¨çš„å¹¶å‘è®¡æ•°å™¨",
                "expected_features": ["struct", "Mutex", "Arc", "impl", "unsafe"]
            }
        ]
    
    def evaluate(self) -> EvaluationResult:
        """è¯„ä¼°ä»£ç ç”Ÿæˆèƒ½åŠ›"""
        print("Evaluating code generation...")
        
        total_score = 0
        examples = []
        
        for i, test_case in enumerate(tqdm(self.test_cases, desc="Code Generation")):
            prompt = f"è¯·ç”¨{test_case['language']}è¯­è¨€{test_case['task']}ã€‚è¦æ±‚ä»£ç æ¸…æ™°ã€å®Œæ•´ã€å¯æ‰§è¡Œã€‚"
            
            try:
                response = self.evaluator.generate_response(prompt, max_length=1536)
                
                # è¯„ä¼°ç”Ÿæˆçš„ä»£ç 
                score = self._score_code_generation(response, test_case['expected_features'], test_case['language'])
                total_score += score
                
                examples.append({
                    "test_case": i + 1,
                    "language": test_case['language'],
                    "task": test_case['task'],
                    "generated_code": response,
                    "score": score,
                    "expected_features": test_case['expected_features']
                })
                
            except Exception as e:
                print(f"Error in generation test case {i+1}: {e}")
                examples.append({
                    "test_case": i + 1,
                    "error": str(e),
                    "score": 0
                })
        
        avg_score = total_score / len(self.test_cases)
        
        return EvaluationResult(
            task="code_generation",
            score=avg_score,
            details={
                "total_cases": len(self.test_cases),
                "average_score": avg_score,
                "languages": [tc['language'] for tc in self.test_cases]
            },
            examples=examples,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
    
    def _score_code_generation(self, code: str, expected_features: List[str], language: str) -> float:
        """è¯„åˆ†ç”Ÿæˆçš„ä»£ç """
        # ç‰¹å¾åŒ¹é…åº¦
        feature_score = sum(1 for feature in expected_features if feature in code) / len(expected_features)
        
        # ä»£ç å—æ£€æµ‹
        has_code_block = "```" in code or any(
            keyword in code for keyword in ["def ", "class ", "func ", "public ", "private "]
        )
        code_block_score = 1.0 if has_code_block else 0.3
        
        # è¯­æ³•ç»“æ„æ£€æµ‹
        syntax_patterns = {
            "Python": ["def ", "class ", "if ", "for ", "import "],
            "Java": ["public ", "class ", "private ", "import ", "{"],
            "Go": ["func ", "package ", "import ", "type ", "var "],
            "Rust": ["fn ", "struct ", "impl ", "use ", "let "]
        }
        
        patterns = syntax_patterns.get(language, [])
        syntax_score = min(1.0, sum(1 for pattern in patterns if pattern in code) / 3)
        
        # ç»¼åˆè¯„åˆ†
        final_score = (feature_score * 0.4 + code_block_score * 0.3 + syntax_score * 0.3)
        return round(final_score, 2)


class PerformanceEvaluator:
    """æ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self, evaluator: CodeEvaluator):
        self.evaluator = evaluator
    
    def evaluate_latency(self, num_samples: int = 10) -> Dict[str, float]:
        """è¯„ä¼°æ¨ç†å»¶è¿Ÿ"""
        print(f"Evaluating inference latency with {num_samples} samples...")
        
        test_prompt = "è¯·è§£é‡Šä»¥ä¸‹Pythonä»£ç çš„åŠŸèƒ½ï¼š\n\n```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n```"
        
        latencies = []
        
        for i in tqdm(range(num_samples), desc="Latency Test"):
            start_time = time.time()
            response = self.evaluator.generate_response(test_prompt, max_length=512)
            end_time = time.time()
            
            latencies.append(end_time - start_time)
        
        return {
            "mean_latency": np.mean(latencies),
            "median_latency": np.median(latencies),
            "std_latency": np.std(latencies),
            "min_latency": np.min(latencies),
            "max_latency": np.max(latencies),
            "samples": num_samples
        }
    
    def evaluate_throughput(self, batch_sizes: List[int] = [1, 2, 4]) -> Dict[str, Any]:
        """è¯„ä¼°ååé‡"""
        print("Evaluating throughput...")
        
        results = {}
        test_prompts = [
            "è§£é‡Šè¿™ä¸ªç®—æ³•çš„æ—¶é—´å¤æ‚åº¦",
            "ä¼˜åŒ–è¿™æ®µä»£ç çš„æ€§èƒ½",
            "åˆ†æè¿™ä¸ªè®¾è®¡æ¨¡å¼çš„ä¼˜ç¼ºç‚¹"
        ] * 10  # 30ä¸ªprompts
        
        for batch_size in batch_sizes:
            print(f"Testing batch size: {batch_size}")
            
            start_time = time.time()
            
            # æ‰¹é‡å¤„ç†
            for i in range(0, min(len(test_prompts), 10), batch_size):
                batch = test_prompts[i:i+batch_size]
                for prompt in batch:
                    response = self.evaluator.generate_response(prompt, max_length=256)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            results[f"batch_{batch_size}"] = {
                "total_time": total_time,
                "samples_processed": min(len(test_prompts), 10),
                "samples_per_second": min(len(test_prompts), 10) / total_time
            }
        
        return results


def run_comprehensive_evaluation(model_path: str, output_dir: str = "/codes/evaluation_results"):
    """è¿è¡Œå…¨é¢è¯„ä¼°"""
    print("ğŸ§ª Starting comprehensive evaluation...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = CodeEvaluator(model_path)
    
    # è¿è¡Œå„é¡¹è¯„ä¼°
    results = {}
    
    # 1. ä»£ç ç†è§£è¯„ä¼°
    understanding_evaluator = CodeUnderstandingEvaluator(evaluator)
    results['code_understanding'] = understanding_evaluator.evaluate()
    
    # 2. ä»£ç ç”Ÿæˆè¯„ä¼°
    generation_evaluator = CodeGenerationEvaluator(evaluator)
    results['code_generation'] = generation_evaluator.evaluate()
    
    # 3. æ€§èƒ½è¯„ä¼°
    performance_evaluator = PerformanceEvaluator(evaluator)
    results['performance'] = {
        'latency': performance_evaluator.evaluate_latency(),
        'throughput': performance_evaluator.evaluate_throughput()
    }
    
    # ä¿å­˜ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    result_file = Path(output_dir) / f"evaluation_results_{timestamp}.json"
    
    # è½¬æ¢ç»“æœä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    serializable_results = {}
    for key, value in results.items():
        if isinstance(value, EvaluationResult):
            serializable_results[key] = {
                'task': value.task,
                'score': value.score,
                'details': value.details,
                'examples': value.examples,
                'timestamp': value.timestamp
            }
        else:
            serializable_results[key] = value
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æ€»ç»“
    print("\nğŸ“Š Evaluation Summary:")
    print(f"Code Understanding: {results['code_understanding'].score:.2f}")
    print(f"Code Generation: {results['code_generation'].score:.2f}")
    print(f"Avg Latency: {results['performance']['latency']['mean_latency']:.2f}s")
    print(f"Results saved to: {result_file}")
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate Qwen Code model")
    parser.add_argument("--model_path", type=str, required=True, help="Path to the fine-tuned model")
    parser.add_argument("--output_dir", type=str, default="/codes/evaluation_results", help="Output directory for results")
    
    args = parser.parse_args()
    
    run_comprehensive_evaluation(args.model_path, args.output_dir)
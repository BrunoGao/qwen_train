#!/bin/bash
# Qwen 2.5 Code A40 ËÆ≠ÁªÉÁéØÂ¢ÉÈÖçÁΩÆËÑöÊú¨

set -e

echo "üöÄ Setting up Qwen 2.5 Code training environment for A40 GPUs..."

# Ê£ÄÊü•CUDAÁâàÊú¨
check_cuda() {
    echo "üìã Checking CUDA environment..."
    if command -v nvidia-smi &> /dev/null; then
        nvidia-smi
        echo "‚úÖ NVIDIA drivers detected"
    else
        echo "‚ùå NVIDIA drivers not found. Please install CUDA drivers first."
        exit 1
    fi
    
    if command -v nvcc &> /dev/null; then
        nvcc --version
        echo "‚úÖ CUDA toolkit detected"
    else
        echo "‚ö†Ô∏è  CUDA toolkit not found. Some features may not work."
    fi
}

# ÂàõÂª∫condaÁéØÂ¢É
setup_conda_env() {
    echo "üêç Setting up Python environment..."
    
    # Ê£ÄÊü•condaÊòØÂê¶Â≠òÂú®
    if ! command -v conda &> /dev/null; then
        echo "‚ùå Conda not found. Please install Anaconda or Miniconda first."
        exit 1
    fi
    
    # ÂàõÂª∫Êñ∞ÁéØÂ¢É
    ENV_NAME="qwen_code_training"
    echo "Creating conda environment: $ENV_NAME"
    
    conda create -n $ENV_NAME python=3.10 -y
    
    echo "Environment created. To activate: conda activate $ENV_NAME"
}

# ÂÆâË£ÖPyTorchÂíåÁõ∏ÂÖ≥‰æùËµñ
install_pytorch() {
    echo "üî• Installing PyTorch and CUDA dependencies..."
    
    # ÊøÄÊ¥ªÁéØÂ¢É
    source $(conda info --base)/etc/profile.d/conda.sh
    conda activate qwen_code_training
    
    # ÂÆâË£ÖPyTorch with CUDA 12.1
    pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121
    
    echo "‚úÖ PyTorch installed successfully"
}

# ÂÆâË£ÖËÆ≠ÁªÉÊ°ÜÊû∂
install_training_frameworks() {
    echo "‚ö° Installing training frameworks..."
    
    source $(conda info --base)/etc/profile.d/conda.sh
    conda activate qwen_code_training
    
    # Core training dependencies
    pip install transformers==4.36.0
    pip install accelerate==0.25.0
    pip install deepspeed==0.12.6
    pip install peft==0.7.1
    
    # Flash Attention for A40
    pip install flash-attn==2.5.0 --no-build-isolation
    
    # Data processing
    pip install datasets==2.15.0
    pip install tokenizers==0.15.0
    
    # Monitoring and evaluation
    pip install wandb==0.16.0
    pip install tensorboard==2.15.0
    
    # Utilities
    pip install tqdm==4.66.0
    pip install pandas==2.1.0
    pip install numpy==1.24.0
    pip install scipy==1.11.0
    
    # Development tools
    pip install ipython==8.17.0
    pip install jupyter==1.0.0
    pip install matplotlib==3.8.0
    pip install seaborn==0.13.0
    
    echo "‚úÖ Training frameworks installed successfully"
}

# ÂÆâË£Ö‰ºòÂåñÂ∑•ÂÖ∑
install_optimization_tools() {
    echo "üõ†Ô∏è Installing optimization tools..."
    
    source $(conda info --base)/etc/profile.d/conda.sh
    conda activate qwen_code_training
    
    # Compilation optimization
    pip install ninja==1.11.0
    pip install packaging==23.2
    
    # Memory optimization
    pip install psutil==5.9.0
    pip install gpustat==1.1.1
    
    # Distributed training
    pip install mpi4py==3.1.5
    
    echo "‚úÖ Optimization tools installed successfully"
}

# ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè
setup_environment_variables() {
    echo "üîß Setting up environment variables..."
    
    # ÂàõÂª∫ÁéØÂ¢ÉÂèòÈáèÊñá‰ª∂
    cat > /codes/training_env.sh << 'EOF'
#!/bin/bash
# Qwen Code Training Environment Variables

# CUDA Configuration
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export NCCL_P2P_DISABLE=1
export NCCL_IB_DISABLE=1
export NCCL_SOCKET_IFNAME=lo

# Memory optimization
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
export TOKENIZERS_PARALLELISM=false

# Distributed training
export MASTER_ADDR="localhost"
export MASTER_PORT="29500"
export WORLD_SIZE=1
export RANK=0

# Paths
export QWEN_MODEL_PATH="/path/to/qwen2.5-7b-instruct"
export TRAINING_DATA_PATH="/codes/processed_training_data"
export OUTPUT_DIR="/codes/outputs"

# Optimization
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8

# Wandb (optional)
export WANDB_PROJECT="qwen-code-training"
export WANDB_ENTITY="your-team"

echo "Environment variables set for Qwen Code training"
EOF
    
    chmod +x /codes/training_env.sh
    echo "‚úÖ Environment variables configured in /codes/training_env.sh"
}

# ÂàõÂª∫DeepSpeedÈÖçÁΩÆ
create_deepspeed_config() {
    echo "‚öôÔ∏è Creating DeepSpeed configuration..."
    
    cat > /codes/deepspeed_config.json << 'EOF'
{
  "train_batch_size": 64,
  "train_micro_batch_size_per_gpu": 2,
  "gradient_accumulation_steps": 8,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 2e-5,
      "betas": [0.9, 0.95],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  },
  "scheduler": {
    "type": "WarmupLR",
    "params": {
      "warmup_min_lr": 0,
      "warmup_max_lr": 2e-5,
      "warmup_num_steps": 1000
    }
  },
  "fp16": {
    "enabled": false,
    "auto_cast": false,
    "loss_scale": 0,
    "initial_scale_power": 16,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "bf16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2,
    "allgather_partitions": true,
    "allgather_bucket_size": 5e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 5e8,
    "contiguous_gradients": true,
    "cpu_offload": false
  },
  "gradient_clipping": 1.0,
  "steps_per_print": 100,
  "wall_clock_breakdown": false,
  "dump_state": false
}
EOF
    
    echo "‚úÖ DeepSpeed configuration created"
}

# ÂàõÂª∫ÂêØÂä®ËÑöÊú¨
create_launch_scripts() {
    echo "üöÄ Creating training launch scripts..."
    
    # ÂçïÊú∫Â§öÂç°ËÆ≠ÁªÉËÑöÊú¨
    cat > /codes/launch_single_node.sh << 'EOF'
#!/bin/bash
# ÂçïÊú∫Â§öÂç°ËÆ≠ÁªÉÂêØÂä®ËÑöÊú¨

source /codes/training_env.sh

# Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ
echo "Starting data preprocessing..."
python /codes/data_preprocessing.py

# ÂêØÂä®ËÆ≠ÁªÉ
echo "Starting Qwen 2.5 Code training on single node..."
torchrun --nproc_per_node=2 \
         --master_port=29500 \
         /codes/train_qwen_code.py \
         --model_name_or_path $QWEN_MODEL_PATH \
         --data_path $TRAINING_DATA_PATH \
         --output_dir $OUTPUT_DIR \
         --num_train_epochs 3 \
         --per_device_train_batch_size 2 \
         --gradient_accumulation_steps 8 \
         --learning_rate 2e-5 \
         --save_steps 1000 \
         --logging_steps 100 \
         --deepspeed /codes/deepspeed_config.json \
         --bf16 true \
         --gradient_checkpointing true \
         --dataloader_num_workers 4 \
         --report_to wandb \
         --run_name "qwen-2.5-7b-code-single-node"
EOF
    
    # Â§öÊú∫ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉËÑöÊú¨
    cat > /codes/launch_multi_node.sh << 'EOF'
#!/bin/bash
# Â§öÊú∫ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÂêØÂä®ËÑöÊú¨

source /codes/training_env.sh

# Ëé∑ÂèñËäÇÁÇπ‰ø°ÊÅØ
NODE_RANK=${NODE_RANK:-0}
NNODES=${NNODES:-4}
MASTER_ADDR=${MASTER_ADDR:-"192.168.1.100"}

echo "Starting multi-node training..."
echo "Node rank: $NODE_RANK"
echo "Total nodes: $NNODES"
echo "Master address: $MASTER_ADDR"

# ÂêØÂä®ËÆ≠ÁªÉ
torchrun --nnodes=$NNODES \
         --node_rank=$NODE_RANK \
         --master_addr=$MASTER_ADDR \
         --master_port=29500 \
         --nproc_per_node=2 \
         /codes/train_qwen_code.py \
         --model_name_or_path $QWEN_MODEL_PATH \
         --data_path $TRAINING_DATA_PATH \
         --output_dir $OUTPUT_DIR \
         --num_train_epochs 3 \
         --per_device_train_batch_size 2 \
         --gradient_accumulation_steps 8 \
         --learning_rate 2e-5 \
         --save_steps 500 \
         --logging_steps 50 \
         --deepspeed /codes/deepspeed_config.json \
         --bf16 true \
         --gradient_checkpointing true \
         --dataloader_num_workers 4 \
         --report_to wandb \
         --run_name "qwen-2.5-7b-code-multi-node"
EOF
    
    chmod +x /codes/launch_*.sh
    echo "‚úÖ Launch scripts created"
}

# ÂàõÂª∫ÁõëÊéßËÑöÊú¨
create_monitoring_scripts() {
    echo "üìä Creating monitoring scripts..."
    
    cat > /codes/monitor_training.py << 'EOF'
#!/usr/bin/env python3
"""
ËÆ≠ÁªÉÁõëÊéßËÑöÊú¨
"""
import time
import psutil
import subprocess
import json
from datetime import datetime

def get_gpu_info():
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,memory.used,memory.total,utilization.gpu,temperature.gpu', '--format=csv,noheader,nounits'], capture_output=True, text=True)
        lines = result.stdout.strip().split('\n')
        gpu_info = []
        for line in lines:
            parts = line.split(', ')
            gpu_info.append({
                'index': int(parts[0]),
                'name': parts[1],
                'memory_used': int(parts[2]),
                'memory_total': int(parts[3]),
                'utilization': int(parts[4]),
                'temperature': int(parts[5])
            })
        return gpu_info
    except:
        return []

def monitor_system():
    while True:
        timestamp = datetime.now().isoformat()
        
        # System info
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        # GPU info
        gpu_info = get_gpu_info()
        
        # Create monitoring data
        monitor_data = {
            'timestamp': timestamp,
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_used_gb': memory.used / (1024**3),
            'memory_total_gb': memory.total / (1024**3),
            'gpu_info': gpu_info
        }
        
        print(f"[{timestamp}] CPU: {cpu_percent:.1f}% | Memory: {memory.percent:.1f}% | GPUs: {len(gpu_info)}")
        for gpu in gpu_info:
            print(f"  GPU{gpu['index']}: {gpu['utilization']}% | {gpu['memory_used']}/{gpu['memory_total']}MB | {gpu['temperature']}¬∞C")
        
        # Save to log file
        with open('/codes/logs/system_monitor.jsonl', 'a') as f:
            f.write(json.dumps(monitor_data) + '\n')
        
        time.sleep(30)  # Monitor every 30 seconds

if __name__ == "__main__":
    import os
    os.makedirs('/codes/logs', exist_ok=True)
    monitor_system()
EOF
    
    chmod +x /codes/monitor_training.py
    echo "‚úÖ Monitoring script created"
}

# ÂàõÂª∫ÊµãËØïËÑöÊú¨
create_test_scripts() {
    echo "üß™ Creating test scripts..."
    
    cat > /codes/test_environment.py << 'EOF'
#!/usr/bin/env python3
"""
ÁéØÂ¢ÉÊµãËØïËÑöÊú¨
"""
import torch
import transformers
import deepspeed
import datasets

def test_cuda():
    print("üîç Testing CUDA...")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"GPU count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")

def test_torch():
    print("\nüî• Testing PyTorch...")
    x = torch.randn(2, 3, device='cuda' if torch.cuda.is_available() else 'cpu')
    y = torch.randn(3, 4, device='cuda' if torch.cuda.is_available() else 'cpu')
    z = torch.mm(x, y)
    print(f"Matrix multiplication test: {z.shape}")
    print("‚úÖ PyTorch working correctly")

def test_transformers():
    print("\nü§ñ Testing Transformers...")
    print(f"Transformers version: {transformers.__version__}")
    
    try:
        from transformers import AutoTokenizer
        print("‚úÖ Transformers import successful")
    except Exception as e:
        print(f"‚ùå Transformers import failed: {e}")

def test_deepspeed():
    print("\n‚ö° Testing DeepSpeed...")
    print(f"DeepSpeed version: {deepspeed.__version__}")
    
    try:
        # Simple DeepSpeed test
        print("‚úÖ DeepSpeed import successful")
    except Exception as e:
        print(f"‚ùå DeepSpeed test failed: {e}")

def test_datasets():
    print("\nüìä Testing Datasets...")
    print(f"Datasets version: {datasets.__version__}")
    
    try:
        from datasets import Dataset
        test_data = Dataset.from_dict({"text": ["hello", "world"]})
        print(f"Test dataset created with {len(test_data)} examples")
        print("‚úÖ Datasets working correctly")
    except Exception as e:
        print(f"‚ùå Datasets test failed: {e}")

def test_flash_attention():
    print("\n‚ö° Testing Flash Attention...")
    try:
        import flash_attn
        print(f"Flash Attention version: {flash_attn.__version__}")
        print("‚úÖ Flash Attention available")
    except ImportError:
        print("‚ö†Ô∏è  Flash Attention not available (may affect training speed)")

if __name__ == "__main__":
    print("üß™ Testing Qwen Code training environment...\n")
    
    test_cuda()
    test_torch()
    test_transformers()
    test_deepspeed()
    test_datasets()
    test_flash_attention()
    
    print("\nüéâ Environment test completed!")
EOF
    
    chmod +x /codes/test_environment.py
    echo "‚úÖ Test script created"
}

# ‰∏ªÂÆâË£ÖÊµÅÁ®ã
main() {
    echo "Starting Qwen 2.5 Code training environment setup..."
    
    check_cuda
    setup_conda_env
    install_pytorch
    install_training_frameworks
    install_optimization_tools
    setup_environment_variables
    create_deepspeed_config
    create_launch_scripts
    create_monitoring_scripts
    create_test_scripts
    
    echo ""
    echo "üéâ Environment setup completed successfully!"
    echo ""
    echo "Next steps:"
    echo "1. Activate the environment: conda activate qwen_code_training"
    echo "2. Source environment variables: source /codes/training_env.sh"
    echo "3. Test the environment: python /codes/test_environment.py"
    echo "4. Preprocess the data: python /codes/data_preprocessing.py"
    echo "5. Start training: bash /codes/launch_single_node.sh"
    echo ""
    echo "For multi-node training, run on each node:"
    echo "  NODE_RANK=<rank> NNODES=<total> MASTER_ADDR=<ip> bash /codes/launch_multi_node.sh"
    echo ""
    echo "Monitor training progress:"
    echo "  python /codes/monitor_training.py"
}

# ËøêË°å‰∏ªÂáΩÊï∞
main
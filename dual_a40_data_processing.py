#!/usr/bin/env python3
"""
åŒA40ä¼˜åŒ–çš„æ•°æ®é¢„å¤„ç†ç®¡é“
é’ˆå¯¹æœ‰é™æ˜¾å­˜ä¼˜åŒ–çš„æ•°æ®å¤„ç†ç­–ç•¥
"""

import os
import json
import multiprocessing as mp
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import hashlib
import random

from tqdm import tqdm
import pandas as pd
from datasets import Dataset, load_from_disk
from transformers import AutoTokenizer


@dataclass
class DualA40Config:
    """åŒA40é…ç½®å‚æ•°"""
    max_seq_length: int = 4096
    min_seq_length: int = 256  # è¿‡æ»¤è¿‡çŸ­æ ·æœ¬
    target_samples: int = 100000  # ç›®æ ‡æ ·æœ¬æ•°ï¼Œæ§åˆ¶å†…å­˜
    batch_size: int = 1000  # å¤„ç†æ‰¹å¤§å°
    memory_limit_gb: int = 32  # æ•°æ®å¤„ç†å†…å­˜é™åˆ¶
    quality_threshold: float = 0.7  # è´¨é‡é˜ˆå€¼
    max_workers: int = 8


class OptimizedCodeProcessor:
    """é’ˆå¯¹åŒA40ä¼˜åŒ–çš„ä»£ç å¤„ç†å™¨"""
    
    def __init__(self, config: DualA40Config, tokenizer_path: Optional[str] = None):
        self.config = config
        self.tokenizer = None
        
        if tokenizer_path and os.path.exists(tokenizer_path):
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                print(f"âœ… Tokenizer loaded from {tokenizer_path}")
            except Exception as e:
                print(f"âš ï¸ Failed to load tokenizer: {e}")
        
        # ä¼˜åŒ–çš„è¯­è¨€æ‰©å±•æ˜ å°„
        self.language_map = {
            '.py': 'Python', '.java': 'Java', '.go': 'Go', '.rs': 'Rust',
            '.cpp': 'C++', '.c': 'C', '.h': 'C/C++', '.hpp': 'C++',
            '.js': 'JavaScript', '.ts': 'TypeScript'
        }
        
        # é«˜ä¼˜å…ˆçº§æ–‡ä»¶ç±»å‹ï¼ˆè®­ç»ƒæ•ˆæœæ›´å¥½ï¼‰
        self.priority_extensions = {'.py', '.java', '.go', '.rs', '.cpp'}
        
    def extract_high_quality_files(self, repo_path: str, repo_info: Dict) -> List[str]:
        """æå–é«˜è´¨é‡ä»£ç æ–‡ä»¶"""
        repo_path = Path(repo_path)
        if not repo_path.exists():
            return []
        
        code_files = []
        priority_files = []
        
        # éå†æ–‡ä»¶
        for file_path in repo_path.rglob('*'):
            if not file_path.is_file():
                continue
                
            # æ£€æŸ¥æ‰©å±•å
            if file_path.suffix not in self.language_map:
                continue
            
            # è¿‡æ»¤ä¸éœ€è¦çš„ç›®å½•å’Œæ–‡ä»¶
            if self._should_skip_file(str(file_path)):
                continue
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å° (5KB - 200KB)
            try:
                file_size = file_path.stat().st_size
                if file_size < 5 * 1024 or file_size > 200 * 1024:
                    continue
            except OSError:
                continue
            
            file_str = str(file_path)
            if file_path.suffix in self.priority_extensions:
                priority_files.append(file_str)
            else:
                code_files.append(file_str)
        
        # ä¼˜å…ˆè¿”å›é«˜ä¼˜å…ˆçº§æ–‡ä»¶
        all_files = priority_files + code_files
        
        # æ ¹æ®ä»“åº“è´¨é‡åˆ†æ•°é‡‡æ ·
        quality_score = repo_info.get('code_quality_score', 8.0)
        sample_ratio = min(1.0, quality_score / 9.0)  # è´¨é‡è¶Šé«˜é‡‡æ ·æ¯”ä¾‹è¶Šå¤§
        
        max_files = int(len(all_files) * sample_ratio)
        selected_files = all_files[:max_files]
        
        print(f"Repository {repo_info['name']}: {len(selected_files)}/{len(all_files)} files selected")
        return selected_files

    def _should_skip_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦è·³è¿‡æ–‡ä»¶"""
        file_path_lower = file_path.lower()
        
        # è·³è¿‡çš„ç›®å½•
        skip_dirs = {
            'test', 'tests', 'testing', 'spec', 'specs', 'mock', 'mocks',
            'example', 'examples', 'demo', 'demos', 'benchmark', 'benchmarks',
            'node_modules', 'target', 'build', '__pycache__', '.git',
            'vendor', 'third_party', 'external', 'lib', 'libs', 'dist'
        }
        
        # è·³è¿‡çš„æ–‡ä»¶åæ¨¡å¼
        skip_patterns = {
            'test_', '_test.', 'spec_', '_spec.', 'mock_', '_mock.',
            'example_', '_example.', 'demo_', '_demo.', 'benchmark_', '_benchmark.',
            '.min.', '.bundle.', '.dist.'
        }
        
        # æ£€æŸ¥ç›®å½•
        for skip_dir in skip_dirs:
            if f'/{skip_dir}/' in file_path_lower or file_path_lower.endswith(f'/{skip_dir}'):
                return True
        
        # æ£€æŸ¥æ–‡ä»¶å
        filename = Path(file_path).name.lower()
        for pattern in skip_patterns:
            if pattern in filename:
                return True
        
        return False

    def process_file_optimized(self, file_info: Tuple[str, Dict]) -> Optional[Dict]:
        """ä¼˜åŒ–çš„æ–‡ä»¶å¤„ç†ï¼Œå†…å­˜å‹å¥½"""
        file_path, repo_info = file_info
        
        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().strip()
            
            # å¿«é€Ÿè´¨é‡æ£€æŸ¥
            if not self._quick_quality_check(content):
                return None
            
            # æ£€æµ‹è¯­è¨€
            language = self._detect_language(file_path)
            
            # ç”Ÿæˆè®­ç»ƒæ ·æœ¬
            sample = self._generate_optimized_sample(content, language, file_path, repo_info)
            
            # é•¿åº¦æ£€æŸ¥
            if self.tokenizer:
                token_count = len(self.tokenizer.encode(sample['formatted_text']))
                if token_count < self.config.min_seq_length or token_count > self.config.max_seq_length:
                    return None
                sample['token_count'] = token_count
            else:
                # ä¼°ç®—tokenæ•°é‡ (1 token â‰ˆ 4 å­—ç¬¦)
                estimated_tokens = len(sample['formatted_text']) // 4
                if estimated_tokens < self.config.min_seq_length or estimated_tokens > self.config.max_seq_length:
                    return None
                sample['token_count'] = estimated_tokens
            
            return sample
            
        except Exception as e:
            return None

    def _quick_quality_check(self, content: str) -> bool:
        """å¿«é€Ÿè´¨é‡æ£€æŸ¥"""
        lines = content.split('\n')
        
        # åŸºæœ¬é•¿åº¦
        if len(content) < 500 or len(lines) < 15:
            return False
        
        # ç©ºè¡Œæ¯”ä¾‹
        empty_lines = sum(1 for line in lines if not line.strip())
        if empty_lines / len(lines) > 0.6:
            return False
        
        # å¹³å‡è¡Œé•¿åº¦
        avg_line_length = len(content) / len(lines)
        if avg_line_length < 10 or avg_line_length > 200:
            return False
        
        return True

    def _detect_language(self, file_path: str) -> str:
        """æ£€æµ‹ç¼–ç¨‹è¯­è¨€"""
        ext = Path(file_path).suffix.lower()
        return self.language_map.get(ext, 'Unknown')

    def _generate_optimized_sample(self, content: str, language: str, file_path: str, repo_info: Dict) -> Dict:
        """ç”Ÿæˆä¼˜åŒ–çš„è®­ç»ƒæ ·æœ¬"""
        
        # ç®€åŒ–çš„æ ·æœ¬ç”Ÿæˆï¼Œå‡å°‘è®¡ç®—å¼€é”€
        sample_types = [
            "è¯·åˆ†æä»¥ä¸‹{language}ä»£ç çš„åŠŸèƒ½å’Œå®ç°:",
            "è§£é‡Šè¿™æ®µ{language}ä»£ç çš„ä¸»è¦é€»è¾‘:",
            "è¯·å¯¹è¿™ä¸ª{language}ä»£ç è¿›è¡Œä»£ç å®¡æŸ¥:",
            "åˆ†æè¿™æ®µ{language}ä»£ç çš„ç®—æ³•å¤æ‚åº¦:"
        ]
        
        instruction = random.choice(sample_types).format(language=language)
        
        # ç®€åŒ–çš„ä»£ç åˆ†æ
        lines = content.split('\n')
        analysis_parts = [f"è¿™æ˜¯ä¸€ä¸ª{language}ä»£ç æ–‡ä»¶ï¼ŒåŒ…å«{len(lines)}è¡Œä»£ç ã€‚"]
        
        # åŸºæœ¬ç‰¹å¾åˆ†æ
        if language == 'Python':
            functions = len([l for l in lines if l.strip().startswith('def ')])
            classes = len([l for l in lines if l.strip().startswith('class ')])
            if functions > 0:
                analysis_parts.append(f"å®šä¹‰äº†{functions}ä¸ªå‡½æ•°")
            if classes > 0:
                analysis_parts.append(f"å®šä¹‰äº†{classes}ä¸ªç±»")
        
        elif language == 'Java':
            methods = len([l for l in lines if 'public ' in l and '(' in l])
            classes = len([l for l in lines if 'class ' in l])
            if methods > 0:
                analysis_parts.append(f"åŒ…å«{methods}ä¸ªæ–¹æ³•")
            if classes > 0:
                analysis_parts.append(f"å®šä¹‰äº†{classes}ä¸ªç±»")
        
        analysis = 'ï¼Œ'.join(analysis_parts) + "ã€‚"
        
        # æ„å»ºQwenå¯¹è¯æ ¼å¼
        formatted_text = (
            f"<|im_start|>system\n"
            f"ä½ æ˜¯ä¸€ä¸ªä»£ç åˆ†æä¸“å®¶ã€‚<|im_end|>\n"
            f"<|im_start|>user\n"
            f"{instruction}\n\n"
            f"```{language.lower()}\n"
            f"{content}\n"
            f"```<|im_end|>\n"
            f"<|im_start|>assistant\n"
            f"{analysis}<|im_end|>"
        )
        
        return {
            'instruction': instruction,
            'input': content,
            'output': analysis,
            'formatted_text': formatted_text,
            'language': language,
            'repo_name': repo_info['name'],
            'priority': repo_info['priority'],
            'quality_score': repo_info['code_quality_score'],
            'file_path': file_path
        }


def process_repositories_dual_a40(config: DualA40Config) -> Dataset:
    """åŒA40ä¼˜åŒ–çš„ä»“åº“å¤„ç†"""
    
    processor = OptimizedCodeProcessor(config)
    
    # åŠ è½½ä»“åº“é…ç½®
    with open("/codes/qwen25_code_training_repositories.json", 'r', encoding='utf-8') as f:
        repo_config = json.load(f)
    
    repo_map = {repo['name']: repo for repo in repo_config['repositories']}
    
    print(f"ğŸ¯ Target samples: {config.target_samples}")
    print(f"ğŸ“ Sequence length: {config.min_seq_length}-{config.max_seq_length}")
    
    all_file_infos = []
    
    # æŒ‰ä¼˜å…ˆçº§å¤„ç†ä»“åº“
    for priority in ['high_priority', 'medium_priority']:
        priority_path = Path("/codes/repositories") / priority
        if not priority_path.exists():
            continue
        
        print(f"\nğŸ“‚ Processing {priority} repositories...")
        
        for repo_dir in priority_path.rglob('*'):
            if not repo_dir.is_dir() or repo_dir.name in {'.git', '__pycache__'}:
                continue
                
            # æ‰¾åˆ°å®é™…çš„ä»“åº“ç›®å½•ï¼ˆè·³è¿‡è¯­è¨€åˆ†ç±»ç›®å½•ï¼‰
            repo_name = None
            for possible_name in repo_map.keys():
                if possible_name in str(repo_dir):
                    repo_name = possible_name
                    break
            
            if not repo_name:
                continue
                
            repo_info = repo_map[repo_name].copy()
            repo_info['priority'] = priority.replace('_priority', '').upper()
            
            # æå–é«˜è´¨é‡æ–‡ä»¶
            code_files = processor.extract_high_quality_files(str(repo_dir), repo_info)
            
            # æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—
            for file_path in code_files:
                all_file_infos.append((file_path, repo_info))
        
        # å¦‚æœå·²ç»æœ‰è¶³å¤Ÿçš„æ–‡ä»¶ï¼Œå¯ä»¥æå‰åœæ­¢
        if len(all_file_infos) > config.target_samples * 2:
            break
    
    print(f"\nğŸ“Š Total files to process: {len(all_file_infos)}")
    
    # éšæœºæ‰“ä¹±ä»¥ç¡®ä¿å¤šæ ·æ€§
    random.shuffle(all_file_infos)
    
    # åˆ†æ‰¹å¤„ç†ä»¥æ§åˆ¶å†…å­˜ä½¿ç”¨
    all_samples = []
    batch_size = config.batch_size
    
    with mp.Pool(processes=min(config.max_workers, mp.cpu_count())) as pool:
        for i in tqdm(range(0, len(all_file_infos), batch_size), desc="Processing batches"):
            batch = all_file_infos[i:i + batch_size]
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            batch_results = pool.map(processor.process_file_optimized, batch)
            
            # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
            valid_samples = [sample for sample in batch_results if sample is not None]
            all_samples.extend(valid_samples)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æ•°é‡
            if len(all_samples) >= config.target_samples:
                print(f"ğŸ¯ Reached target samples: {len(all_samples)}")
                break
            
            # å†…å­˜æ§åˆ¶ï¼šæ¯å¤„ç†5ä¸ªæ‰¹æ¬¡æ¸…ç†ä¸€æ¬¡
            if (i // batch_size + 1) % 5 == 0:
                print(f"ğŸ“ˆ Current samples: {len(all_samples)}")
    
    # æˆªå–åˆ°ç›®æ ‡æ•°é‡
    if len(all_samples) > config.target_samples:
        all_samples = all_samples[:config.target_samples]
    
    print(f"âœ… Final dataset size: {len(all_samples)}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {}
    for sample in all_samples:
        lang = sample['language']
        stats[lang] = stats.get(lang, 0) + 1
    
    print("\nğŸ“Š Language distribution:")
    for lang, count in sorted(stats.items(), key=lambda x: x[1], reverse=True):
        percentage = count / len(all_samples) * 100
        print(f"  {lang}: {count} samples ({percentage:.1f}%)")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = Dataset.from_list(all_samples)
    return dataset


def save_dual_a40_dataset(dataset: Dataset, output_path: str = "/codes/dual_a40_training_data"):
    """ä¿å­˜åŒA40ä¼˜åŒ–æ•°æ®é›†"""
    print(f"ğŸ’¾ Saving dataset to {output_path}...")
    
    # ä¿å­˜æ•°æ®é›†
    dataset.save_to_disk(output_path)
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    df = dataset.to_pandas()
    stats = {
        'total_samples': len(dataset),
        'avg_token_count': int(df['token_count'].mean()),
        'token_distribution': {
            'min': int(df['token_count'].min()),
            'max': int(df['token_count'].max()),
            'std': float(df['token_count'].std())
        },
        'language_distribution': dict(df['language'].value_counts()),
        'quality_score_avg': float(df['quality_score'].mean()),
        'priority_distribution': dict(df['priority'].value_counts()),
        'estimated_training_tokens': int(df['token_count'].sum())
    }
    
    stats_path = Path(output_path) / "dataset_stats.json"
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Dataset saved: {stats['total_samples']} samples")
    print(f"ğŸ“Š Average tokens per sample: {stats['avg_token_count']}")
    print(f"ğŸ¯ Total training tokens: {stats['estimated_training_tokens']:,}")
    
    return stats


if __name__ == "__main__":
    print("ğŸš€ Starting dual A40 optimized data preprocessing...")
    
    # åŒA40é…ç½®
    config = DualA40Config(
        max_seq_length=4096,
        min_seq_length=256,
        target_samples=80000,  # é€‚ä¸­çš„æ ·æœ¬æ•°é‡
        batch_size=500,
        memory_limit_gb=32,
        quality_threshold=0.7,
        max_workers=8
    )
    
    # å¤„ç†æ•°æ®
    dataset = process_repositories_dual_a40(config)
    
    # ä¿å­˜æ•°æ®é›†
    stats = save_dual_a40_dataset(dataset)
    
    print("\nğŸ‰ Dual A40 data preprocessing completed!")
    print(f"ğŸ“ Data location: /codes/dual_a40_training_data")
    print(f"ğŸ† Quality optimized for dual A40 training")